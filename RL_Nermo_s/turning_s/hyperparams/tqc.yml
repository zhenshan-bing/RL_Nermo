NermoMaxVelocity-v0: &nermo-defaults
  n_timesteps: !!float 5e6
  normalize: False
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 300000
  learning_starts: 10000
  batch_size: 256
  gamma: 0.98
  tau: 0.02
  gradient_steps: 8
  train_freq: 8
  ent_coef: 'auto'
  use_sde: False
  sde_sample_freq: -1
  top_quantiles_to_drop_per_net: 5
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"
  env_wrapper:
    - gym.wrappers.rescale_action.RescaleAction:
        min_action: -1
        max_action: 1

NermoFixedVelocity-v0:
  <<: *nermo-defaults

NermoCommandedVelocity-v0:
  <<: *nermo-defaults
  n_timesteps: !!float 10e6

nermo-short-episodes-v0:
  <<: *nermo-defaults

nermo-long-episodes-v0:
  <<: *nermo-defaults